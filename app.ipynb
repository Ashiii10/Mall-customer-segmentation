{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-7HBTR6Se5N",
        "outputId": "93863a2e-9a91-4732-b47b-d8402b2afff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.5-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.36.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.5-py3-none-any.whl (23 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m145.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.2.5 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UljwZMtYcEdW",
        "outputId": "7773ae3f-159c-4765-ec6e-0fcb0a203994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Function to add random feedback column\n",
        "def add_feedback_column(df):\n",
        "    feedback_texts = [\n",
        "        \"Great service!\", \"Not satisfied\", \"Loved the products\",\n",
        "        \"Service can improve\", \"Had a wonderful experience\"\n",
        "    ]\n",
        "    df['feedback'] = [random.choice(feedback_texts) for _ in range(len(df))]\n",
        "    return df\n",
        "\n",
        "# Title of the Streamlit App\n",
        "st.title(\"Mall Customer Segmentation\")\n",
        "\n",
        "# File Upload Section\n",
        "uploaded_file = st.file_uploader(\"Upload CSV file\", type=[\"csv\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Read the uploaded file\n",
        "    df = pd.read_csv(uploaded_file)\n",
        "    st.write(\"Dataset Preview:\")\n",
        "    st.dataframe(df.head())\n",
        "\n",
        "    try:\n",
        "        # Add feedback column if not present\n",
        "        df = add_feedback_column(df)\n",
        "\n",
        "        # Check if feedback column is added and display it\n",
        "        st.write(\"Columns in the dataset after adding feedback:\")\n",
        "        st.write(df.columns)  # Check if 'feedback' column is added\n",
        "\n",
        "        # Display the first few rows of the updated dataframe\n",
        "        st.write(\"Updated Dataset Preview with Feedback Column:\")\n",
        "        st.dataframe(df.head())\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error adding feedback column: {str(e)}\")\n",
        "\n",
        "    # EDA Section\n",
        "    st.subheader(\"Missing Values and Data Types\")\n",
        "    st.write(df.isnull().sum())\n",
        "    st.write(df.dtypes)\n",
        "\n",
        "    numeric_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "    # Create a correlation matrix\n",
        "    corr_matrix = numeric_df.corr()\n",
        "\n",
        "    # Plot the heatmap using Plotly\n",
        "    st.subheader(\"Correlation Heatmap\")\n",
        "    fig = px.imshow(corr_matrix, text_auto=True, color_continuous_scale='RdBu_r', title='Correlation Heatmap')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Feature Scaling\n",
        "    features = df.select_dtypes(include=[np.number]).dropna(axis=1)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "    # Optional: Text feature extraction using Sentence-BERT (if feedback column exists)\n",
        "    if 'feedback' in df.columns:\n",
        "        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "        text_embeddings = model.encode(df['feedback'].tolist())\n",
        "        st.write(f\"Text Embedding Shape: {np.array(text_embeddings).shape}\")\n",
        "    else:\n",
        "        st.warning(\"No 'feedback' column found. Using dummy embeddings.\")\n",
        "        text_embeddings = np.zeros((df.shape[0], 300))\n",
        "\n",
        "    # Combine Numerical and Text Features\n",
        "    combined_features = np.hstack([scaled_features, text_embeddings])\n",
        "\n",
        "    # Finding Optimal K (KMeans)\n",
        "    wcss = []\n",
        "    silhouette_scores = []\n",
        "    K = range(2, 11)\n",
        "\n",
        "    for k in K:\n",
        "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "        kmeans.fit(combined_features)\n",
        "        wcss.append(kmeans.inertia_)\n",
        "        silhouette_scores.append(silhouette_score(combined_features, kmeans.labels_))\n",
        "\n",
        "    # Elbow and Silhouette Plot\n",
        "    st.subheader(\"K-Means Clustering Evaluation Metrics\")\n",
        "    fig = make_subplots(rows=1, cols=2, subplot_titles=('Elbow Method', 'Silhouette Score'))\n",
        "\n",
        "    # Elbow Method (WCSS)\n",
        "    fig.add_trace(go.Scatter(x=list(K), y=wcss, mode='lines+markers', marker=dict(color='blue'), name='WCSS'), row=1, col=1)\n",
        "    # Silhouette Score\n",
        "    fig.add_trace(go.Scatter(x=list(K), y=silhouette_scores, mode='lines+markers', marker=dict(color='green'), name='Silhouette Score'), row=1, col=2)\n",
        "\n",
        "    fig.update_layout(title_text='K-Means Clustering Evaluation Metrics', width=1000, height=400, showlegend=False)\n",
        "    fig.update_xaxes(title_text=\"K (Number of Clusters)\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"WCSS\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"K (Number of Clusters)\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Silhouette Score\", row=1, col=2)\n",
        "\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Perform K-Means Clustering\n",
        "    optimal_k = 4  # Update based on evaluation plots\n",
        "    kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(combined_features)\n",
        "    df['KMeans_Cluster'] = cluster_labels\n",
        "\n",
        "    # PCA for 2D Visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_components = pca.fit_transform(combined_features)\n",
        "\n",
        "    st.subheader(\"PCA Explained Variance\")\n",
        "    for idx, var in enumerate(pca.explained_variance_ratio_):\n",
        "        st.write(f\"Component {idx+1}: {var*100:.2f}%\")\n",
        "\n",
        "    # Plot Cumulative Variance\n",
        "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=list(range(1, len(cumulative_variance)+1)), y=cumulative_variance, mode='lines+markers', line=dict(dash='dash', color='royalblue'), marker=dict(size=8), name='Cumulative Variance'))\n",
        "    fig.update_layout(title='Cumulative Explained Variance by PCA Components', xaxis_title='Number of PCA Components', yaxis_title='Cumulative Variance Explained', width=700, height=500, template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Plot Clusters\n",
        "    pca_df = pd.DataFrame({'PCA1': pca_components[:, 0], 'PCA2': pca_components[:, 1], 'Cluster': cluster_labels})\n",
        "    fig = px.scatter(pca_df, x='PCA1', y='PCA2', color='Cluster', title='Customer Segmentation (KMeans + PCA)', color_continuous_scale='viridis', width=800, height=600)\n",
        "    fig.update_traces(marker=dict(size=8, line=dict(width=1, color='DarkSlateGrey')))\n",
        "    fig.update_layout(legend_title_text='Cluster', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Cluster Profiling\n",
        "    st.subheader(\"Cluster Profiling\")\n",
        "    profile = df.groupby('KMeans_Cluster').mean(numeric_only=True)\n",
        "    st.write(profile)\n",
        "\n",
        "    # Cluster Size Distribution\n",
        "    cluster_counts = df['KMeans_Cluster'].value_counts().sort_index()\n",
        "    cluster_counts_df = pd.DataFrame({'Cluster': cluster_counts.index, 'Count': cluster_counts.values})\n",
        "\n",
        "    fig = px.bar(cluster_counts_df, x='Cluster', y='Count', color='Cluster', title='Number of Customers per Cluster', color_continuous_scale='viridis', width=800, height=500)\n",
        "    fig.update_layout(xaxis_title='Cluster', yaxis_title='Number of Customers', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 3D Scatter Plot of Cluster Counts\n",
        "    st.subheader(\"3D Scatter Plot of Cluster Counts\")\n",
        "\n",
        "    fig = px.scatter_3d(\n",
        "        cluster_counts_df,\n",
        "        x='Cluster',\n",
        "        y='Count',\n",
        "        z=[0] * len(cluster_counts_df),  # Z axis as 0\n",
        "        color='Count',\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='3D Scatter Plot of Cluster Counts',\n",
        "        scene=dict(\n",
        "            xaxis_title='Cluster',\n",
        "            yaxis_title='Count',\n",
        "            zaxis_title='Z'\n",
        "        ),\n",
        "        width=800,\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Pairplot of Features Colored by Cluster\n",
        "    st.subheader(\"Pairplot of Features by Cluster\")\n",
        "\n",
        "    pairplot_data = features.copy()\n",
        "    pairplot_data['Cluster'] = cluster_labels\n",
        "\n",
        "    sns_plot = sns.pairplot(pairplot_data, hue='Cluster', palette='viridis')\n",
        "    plt.suptitle('Pairplot by Cluster', y=1.02)\n",
        "\n",
        "    st.pyplot(plt)\n",
        "    plt.clf()  # Clear figure after displaying\n",
        "\n",
        "    # Save Segmented Dataset\n",
        "    st.subheader(\"Save Segmented Data\")\n",
        "    df.to_csv('customers_segmented.csv', index=False)\n",
        "    st.write(\"Data saved as 'customers_segmented.csv'. You can download it below:\")\n",
        "\n",
        "    # Add download button for segmented data\n",
        "    st.download_button(label=\"Download Segmented Data\", data=df.to_csv(index=False), file_name=\"customers_segmented.csv\", mime=\"text/csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA-cJub-dTuv",
        "outputId": "db1a9644-234e-456e-bd0a-d7a3d0e3c3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://43b3-34-125-9-124.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.9.124:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-04-29 07:46:06.165284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745912766.180536     991 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745912766.185616     991 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-29 07:46:06.201421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-29 07:46:10.185 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-04-29 07:46:20.937 Serialization of dataframe to Arrow table was unsuccessful. Applying automatic fixes for column types to make the dataframe Arrow-compatible.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/dataframe_util.py\", line 822, in convert_pandas_df_to_arrow_bytes\n",
            "    table = pa.Table.from_pandas(df)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"pyarrow/table.pxi\", line 4751, in pyarrow.lib.Table.from_pandas\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/pandas_compat.py\", line 625, in dataframe_to_arrays\n",
            "    arrays = [convert_column(c, f)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/pandas_compat.py\", line 625, in <listcomp>\n",
            "    arrays = [convert_column(c, f)\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/pandas_compat.py\", line 612, in convert_column\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/pandas_compat.py\", line 606, in convert_column\n",
            "    result = pa.array(col, type=type_, from_pandas=True, safe=safe)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"pyarrow/array.pxi\", line 360, in pyarrow.lib.array\n",
            "  File \"pyarrow/array.pxi\", line 87, in pyarrow.lib._ndarray_to_array\n",
            "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
            "pyarrow.lib.ArrowInvalid: (\"Could not convert dtype('int64') with type numpy.dtypes.Int64DType: did not recognize Python value type when inferring an Arrow data type\", 'Conversion failed for column 0 with type object')\n",
            "modules.json: 100% 229/229 [00:00<00:00, 1.40MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 626kB/s]\n",
            "README.md: 100% 3.51k/3.51k [00:00<00:00, 24.7MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 350kB/s]\n",
            "config.json: 100% 629/629 [00:00<00:00, 4.68MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 90.9M/90.9M [00:00<00:00, 276MB/s]\n",
            "tokenizer_config.json: 100% 314/314 [00:00<00:00, 911kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 1.61MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 3.14MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 951kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.35MB/s]\n",
            "2025-04-29 07:46:33.486 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace 'your_authtoken_here' with your actual ngrok authtoken\n",
        "ngrok.set_auth_token(\"2vqRRRi5nGA5kfKqiOB8KaVyVz4_4PahZDHo9iGoaaLi6uBXg\")\n",
        "\n",
        "# Open a ngrok tunnel to the Streamlit app\n",
        "public_url = ngrok.connect(8501)\n",
        "print('Streamlit app is live at:', public_url)\n",
        "\n",
        "# Run Streamlit in background\n",
        "!streamlit run app.py &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfMksddpdXKi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}